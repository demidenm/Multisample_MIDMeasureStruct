---
title: "MID - Measurement Model Code"
author: "<h3>by Michael Demidenko</h3>"
date: "`r format(Sys.time(), '%B %Y')`"
output:
  html_document:
    theme: united
    highlight: tango
    toc: yes
    number_sections: yes
    toc_depth: 2
    toc_float:
      collapsed: yes
      smooth_scroll: yes
    code_folding: hide
    self_contained: yes
  pdf_document:
    toc: yes
    toc_depth: '2'
tags: []
subtitle: <h2><u></u></h2>
---
***
***
```{r include=FALSE}
sess_vers <- version
platform <- sess_vers$platform
r_info <- sess_vers$version.string
```

The platform used to write and generate this .html document is `cat(platform)` on RStudio, `cat(r_info)`. The required packages to run all necessary code chunks are checked and installed using the [pacman](https://www.rdocumentation.org/packages/pacman/versions/0.5.1) R package. `p_load` from the package is used to load and install all listed packages. The details for a number of packages are described before their use. If they are not, the `.rmd` file will provide the list of packages that are installed. 

```{r include=FALSE}
# Automatically check and install necessary packages that were used in R v 4.0

if (!require("pacman")) install.packages("pacman")
pacman::p_load(simsem, plyr, tidyverse,corrplot, reshape2, data.table,ggplot2,esemComp, jpeg,
               Hmisc,nFactors,car,psych, paran, #for EFA
               semTools, semPlot, lavaan, # FOR CFA/ESEM
               parameters, superheat,weights, devtools, heatmaply, # For plotting/tabling
               sirt, msm
               )
```

***
***

# Simulating data {.tabset}

This section is unique only to the Stage 1 registered reported. We are not using any real data, so to ensure that each step is completed correctly, we use simulated data. We can operate under the assumption that all participants will have no missing values given how exclusion criteria are applied and fMRI data are extracted from the BOLD timeseries.
 

In this section, a population model is fit reflecting the theoretical phenomenon that are involved in the reward processing construct. The idea is that there are approach and avoidance (measured constructs) characteristics that are associated with stimuli within/across tasks (Manifest variables). There is no `ground truth` of these processes at the individual. Nevertheless, the variables are important to the broad construct of approach and avoidance that are sub-constructs of the higher-order reward processing construct. In the MID task these constructs are hypothesized to reflect the the multidimensional affective circumplex model. As formalized here, in the MID task the contrasts reflect the manifest variables (i.e., reflective model) as opposed of the other way, whereby the items form the variables predict the construct (i.e., formative model).

## Specify Population Model

Start off by specifying the **population** model. In this scenario, the individual runs load onto the specific Contrast and ROI combinations. Then, the ROIs are loaded onto the factors `approach` and `avoidance`.
The approach and avoidance are specified as *negatively* correlated and the factor variances are fixed to `1`. This is because in the multidimensional circumplex model, when the span of the contrasts are considered, these measures will likely not be orthogonal but instead >90 degree in space.

```{r echo=TRUE, message=FALSE, warning=FALSE}
population_model<-'
# By run loadings for bilateral regions
AWin_v_Neut_L_NAcc =~     .7*AWin_v_Neut_L_NAcc_run1    + .7*AWin_v_Neut_L_NAcc_run2
AWin_v_Neut_L_Insula =~   .7*AWin_v_Neut_L_Insula_run1  + .7*AWin_v_Neut_L_Insula_run2
BWin_v_Neut_L_NAcc =~     .7*BWin_v_Neut_L_NAcc_run1    + .7*BWin_v_Neut_L_NAcc_run2
BWin_v_Neut_L_Insula =~   .7*BWin_v_Neut_L_Insula_run1  + .7*BWin_v_Neut_L_Insula_run2
BWin_v_BLose_L_NAcc =~    .7*BWin_v_BLose_L_NAcc_run1   + .7*BWin_v_BLose_L_NAcc_run2
BWin_v_BLose_L_Insula =~  .7*BWin_v_BLose_L_Insula_run1 + .7*BWin_v_BLose_L_Insula_run2
ALose_v_Neut_L_NAcc =~    .7*ALose_v_Neut_L_NAcc_run1   + .7*ALose_v_Neut_L_NAcc_run2
ALose_v_Neut_L_Insula =~  .7*ALose_v_Neut_L_Insula_run1 + .7*ALose_v_Neut_L_Insula_run2
BLose_v_Neut_L_NAcc =~    .7*BLose_v_Neut_L_NAcc_run1   + .7*BLose_v_Neut_L_NAcc_run2
BLose_v_Neut_L_Insula =~  .7*BLose_v_Neut_L_Insula_run1 + .7*BLose_v_Neut_L_Insula_run2
BLose_v_BWin_L_NAcc =~    .7*BLose_v_BWin_L_NAcc_run1   + .7*BLose_v_BWin_L_NAcc_run2
BLose_v_BWin_L_Insula =~  .7*BLose_v_BWin_L_Insula_run1 + .7*BLose_v_BWin_L_Insula_run2

AWin_v_Neut_R_NAcc =~     .7*AWin_v_Neut_R_NAcc_run1    + .7*AWin_v_Neut_R_NAcc_run2
AWin_v_Neut_R_Insula =~   .7*AWin_v_Neut_R_Insula_run1  + .7*AWin_v_Neut_R_Insula_run2
BWin_v_Neut_R_NAcc =~     .7*BWin_v_Neut_R_NAcc_run1    + .7*BWin_v_Neut_R_NAcc_run2
BWin_v_Neut_R_Insula =~   .7*BWin_v_Neut_R_Insula_run1  + .7*BWin_v_Neut_R_Insula_run2
BWin_v_BLose_R_NAcc =~    .7*BWin_v_BLose_R_NAcc_run1   + .7*BWin_v_BLose_R_NAcc_run2
BWin_v_BLose_R_Insula =~  .7*BWin_v_BLose_R_Insula_run1 + .7*BWin_v_BLose_R_Insula_run2
ALose_v_Neut_R_NAcc =~    .7*ALose_v_Neut_R_NAcc_run1   + .7*ALose_v_Neut_R_NAcc_run2
ALose_v_Neut_R_Insula =~  .7*ALose_v_Neut_R_Insula_run1 + .7*ALose_v_Neut_R_Insula_run2
BLose_v_Neut_R_NAcc =~    .7*BLose_v_Neut_R_NAcc_run1   + .7*BLose_v_Neut_R_NAcc_run2
BLose_v_Neut_R_Insula =~  .7*BLose_v_Neut_R_Insula_run1 + .7*BLose_v_Neut_R_Insula_run2
BLose_v_BWin_R_NAcc =~    .7*BLose_v_BWin_R_NAcc_run1   + .7*BLose_v_BWin_R_NAcc_run2
BLose_v_BWin_R_Insula =~  .7*BLose_v_BWin_R_Insula_run1 + .7*BLose_v_BWin_R_Insula_run2

#Factor item loadings 
Approach =~  .8*AWin_v_Neut_L_NAcc + .8*AWin_v_Neut_R_NAcc + .45*AWin_v_Neut_R_Insula +
            .7*BWin_v_Neut_L_NAcc +   .7*BWin_v_Neut_R_NAcc + .4*BWin_v_Neut_R_Insula +
            .8*BWin_v_BLose_L_NAcc +  .8*BWin_v_BLose_R_NAcc
                
Avoid =~  .8*ALose_v_Neut_L_Insula  + .8*ALose_v_Neut_R_Insula +
          .75*BLose_v_Neut_L_Insula + .75*BLose_v_Neut_R_Insula +
          .8*BLose_v_BWin_L_Insula  + .45*BLose_v_BWin_R_Insula
            
# Factor Covariances 
Approach ~~ -.6*Avoid

# Fixing factor variances
Approach ~~ 1*Approach
Avoid ~~ 1*Avoid

'
```

## General samples

Using the population model, [simsem](https://simsem.org/) is used to create simulated data. This generates a `fake` dataset that is used to pilot the planned Mutligroup CFA, ESEM, EFA and Local SEM models

In this case, 50 repetitions are simulated per model for an *approximate* N sample for each study. Even though the factor variances are specified in the population model as '1', this model fixeds all latent variables using `std.lv = TRUE`.

 1. AHRB N = 108
 2. MLS N = 159
 3. ABCD N = 1000
  
  

```{r echo=TRUE, message=FALSE, warning=FALSE, results='hide'}
set.seed(25151215)
sim_AHRB <- simsem::sim(nRep = 50, model = "lavaan", n = 108, 
           generate = population_model, std.lv = TRUE, lavaanfun = "sem", 
           # std.lv ~ ix the variances of all the latent variables 
           dataOnly=T, meanstructure = FALSE, seed=123)

sim_MLS <- simsem::sim(nRep = 50, model = "lavaan", n = 159, 
                generate = population_model, std.lv = TRUE, lavaanfun = "sem", 
                dataOnly=T, meanstructure = FALSE, seed=123)

sim_ABCD <- simsem::sim(nRep = 50, model = "lavaan", n = 1000, 
               generate = population_model, std.lv = TRUE, lavaanfun = "sem", 
               dataOnly=T, meanstructure = FALSE, seed=123)
```


Average each repetition for each simulated sample. For example, after 50 repetitions of 1000 participants for the population model of ABCD sample, an average estimate is derived using [aaply](https://www.guru99.com/r-apply-sapply-tapply.html). For each study, the `set` variable is created to differentiate which sample the data is associated with (i.e., grouping variable). In this case, `AHRB = 3`, `MLS = 2`, `ABCD = 1`. 

```{r}
sim_AHRB_data <- data.frame(aaply(laply(sim_AHRB, as.matrix), c(2,3), mean))
  sim_AHRB_data$set <-3
  
sim_MLS_data <- data.frame(aaply(laply(sim_MLS, as.matrix), c(2,3), mean))
  sim_MLS_data$set <-2
  
sim_ABCD_data <- data.frame(aaply(laply(sim_ABCD, as.matrix), c(2,3), mean))
  sim_ABCD_data$set <-1
```

Next, row bind the data sets to form one complete data. This equivalent to stacking the datasets and only retaining a single header row (since all variable names are constant). This creates a 1267 x 49 data matrix (48 brain variables + sample label)
```{r}
brain_set <- rbind(sim_AHRB_data,sim_MLS_data,sim_ABCD_data)
```

## Correlation matrix of data

Here, a combination of [rcorr](https://www.rdocumentation.org/packages/Hmisc/versions/4.7-1/topics/rcorr) and [corrplot](https://cran.r-project.org/web/packages/corrplot/vignettes/corrplot-intro.html) are used to visualize the associations among the brain variables in the dataset.

```{r}
# Using Hmisc to create a 24x24 matrix for a list (3) that contains: the pearson's r corr, sample size (N), and significance (p).
Brain_corr = rcorr(as.matrix(subset(brain_set,select=-c(set))), # excluding the set of data related to sample
                   type = "pearson")


# Using corrplot() to create heatmap of the data. 
par(mfrow=c(1,1))
corrplot(Brain_corr$r, type = "upper", 
         order = 'hclust',
         method =  "color", 
         tl.cex = 0.5, tl.col = 'black',
         cl.pos = 'r', tl.pos = 'lt', outline = TRUE,
         col=colorRampPalette(c("navyblue","white","red2"))(100),# colours http://www.stat.columbia.edu/~tzheng/files/Rcolor.pdf
         mar = c(2,.15,.25,.15)#bottom, left, top and right,
         )
```


***
***

# Running [restricted] Multigroup CFA {.tabset}

Run the CFA multi-group analysis for the three datasets. Multi-group CFA tests the measurement invariance across defined groups to determine whether soft and strict invariance criteria are met and the degree to which the derive estimates for an item in one study can be compared to the same item in another sample. In this case, the focus is on the configural (structure) and metric invariance (loadings). In short, this model evaluates whether factor structure and loadings for the approach and avoidance model are invariant (dont significant differ) across the samples. 

Code here is based on measurement invariance models from [Maasen et al. 2019](https://osf.io/j72t4/), Measurement invariance presentation from [Kate Xu](https://users.ugent.be/~yrosseel/lavaan/multiplegroup6Dec2012.pdf) and Multi-group CFA tutorial from [Hirschfeld & Brachel (2014)](https://doi.org/10.7275/qazy-2946).

The issue of multi-group is invariance what is discussed in [Borsboom (2006)](www.doi.org/10.1007/s11336-006-1447-6). In short, (1) Interpretation of group differences on observed scores DEPENDS on the invariance of measurement models & (2) many make conclusions without doing a single test of measurement invariance.

A tutorial on CFA broadly is available from Lizbeth Benzon and Nilam Ram [here](https://quantdev.ssri.psu.edu/tutorials/intro-basic-confirmatory-factor-analysis) and tutorial of measurement invariance (in context of longitudinal data) from Nilam Ram is available [here](https://quantdev.ssri.psu.edu/tutorials/intro-basics-longitudinal-measurement-invariance)


## CFA model 

The below specified model will be used. The number of estimate parameters are fewer and may be more appropriate for the theoretical model. This model may result in few convergence issues if the number of participants ends up to be few and the coefficients/estimates are lower.(see Kline 2015 book on **Principles and Practice of Structural Equation Modeling**).

```{r}
MID_model <-'

# Factor loadings
Approach =~ AWin_v_Neut_L_NAcc_run1  + AWin_v_Neut_R_NAcc_run1  + AWin_v_Neut_R_Insula_run1 +
            BWin_v_Neut_L_NAcc_run1  + BWin_v_Neut_R_NAcc_run1  + BWin_v_Neut_R_Insula_run1 +
            BWin_v_BLose_L_NAcc_run1 + BWin_v_BLose_R_NAcc_run1 +
            AWin_v_Neut_L_NAcc_run2  + AWin_v_Neut_R_NAcc_run2 + AWin_v_Neut_R_Insula_run2 +
            BWin_v_Neut_L_NAcc_run2  + BWin_v_Neut_R_NAcc_run2  + BWin_v_Neut_R_Insula_run2 +
            BWin_v_BLose_L_NAcc_run2 + BWin_v_BLose_R_NAcc_run2 
                
Avoid =~    ALose_v_Neut_L_Insula_run1 + ALose_v_Neut_L_Insula_run1 +
            BLose_v_Neut_L_Insula_run1 + BLose_v_Neut_R_Insula_run1 +
            BLose_v_BWin_L_Insula_run1 + BLose_v_BWin_R_Insula_run1 +
            ALose_v_Neut_L_Insula_run2 + ALose_v_Neut_R_Insula_run2 +
            BLose_v_Neut_L_Insula_run2 + BLose_v_Neut_R_Insula_run2 +
            BLose_v_BWin_L_Insula_run2 + BLose_v_BWin_R_Insula_run2 
'
```


## Running CFA: Three Samples

Below is the CFA model that is used to test the proposed restricted model (see Figure 1 in the manuscript). The CFA fitting procedure is consistent with the description [here](https://lavaan.ugent.be/tutorial/cfa.html). 
For each CFA model, the full sample is filtered for each type sample, e.g. AHRB, MLS, ABCD. The `std.lv= = TRUE` constrain the latent factor variances to *1*. The estimator being used is `MLR`, a maximum likelihood robust estimator. A CFA model is estimated for the complete data (i.e., all three datasets).

```{r}
all_sample <- cfa(model = MID_model, data = brain_set,
               estimator = "MLR", std.lv = TRUE, meanstructure = TRUE)
```

## Fitting Configural CFA

Here, the `configular multigroup model` is fit. As described in [D'Urso et al. (2022)](www.doi.org/10.31234/osf.io/n3f5u) measurement invariance pre-print, the configural model tests: 

**is the structure of the factors is invariannt across the samples ('set'). In other words, if we *a priori* propose a two-factor structure (FA 1 = approch and FA 2 = Avoidance), does this two factor structure represent the between-person variability in the items that reflect the factors across each sample?**

If the variability in one sample suggests a one, three, or four factor structure, this will be degrade the fit statistics. 

A pre-specified CFA model is used to evaluate whether the measures/items that reflect the factor are the same across groups. `group= 'set'` is used to define the grouping variable. All loadings and intercepts are free to vary across groups, and the factor variance is set to '1' via `std.lv = TRUE`

```{r}
configural_cfa <- cfa(model = MID_model, data = brain_set, group = 'set', 
                      estimator = "MLR", std.lv = TRUE, meanstructure = TRUE)
```

## Fitting Metric CFA

After fitting the CFA configurial (factor structure) invariance, if the model fit is not poor, then the next step is to test the metric invariance. Metric invariance tests: 

**are the loadings are consistent across the groups. In other words,are the phenomena (i.e., approach and avoidance) reflected by the same pattern across the measures/items?**

One cause for concern may be that the phenomenon are not invariant across age groups, in that the items/measures (ROIs for a given contrast) do not load in the same manner onto each factor. This 'soft' measure of invariance can determine whether the items functions differ across the items and so cannot be easily compared. 

The model is fit using the same procedure as for configurial invariance with one exception: In metric invariance the loadings group equality constraint is added to the model via `group.equal=c("loadings")`. The model fit statistics are used to evaluate whether the fit is poor. 

```{r}
metric_cfa <-cfa(model = MID_model, data = brain_set, 
                 group = 'set', group.equal=c("loadings"),
                 estimator = "MLR", std.lv = TRUE, meanstructure = TRUE)
```


## Extracting Fit Statistics

Once the above models are fit, the following information is pulled out and saved into a `out` data frame: 

  1. Model name
  2. Chi-square statistics
  3. Model Degrees of Freedom (df)
  4. Model p-value 
  5. RMSEA
  6. CFI
  7. SRMR
  8. AIC
  9. BIC
  
```{r}
# Below selects specific fit data as described in Maassen et al. 2019 OSF. No comparisons are made to compare models at this point.
out <- matrix(NA, ncol = 9, nrow = 4)
colnames(out) <- c("model","chisq","df","pvalue", "rmsea", "cfi", "srmr",
                   "AIC", "BIC")

# save fit measures from models
out[1,2:7] <- round(data.matrix(fitmeasures(all_sample, 
                                            fit.measures = c("chisq","df","pvalue",
                                                             "rmsea", "cfi", "srmr"))), 
                    digits=3)

out[2,2:7] <- round(data.matrix(fitmeasures(configural_cfa, 
                                            fit.measures = c("chisq","df","pvalue",
                                                             "rmsea", "cfi", "srmr"))), 
                            digits=3)

out[3,2:7] <- round(data.matrix(fitmeasures(metric_cfa, 
                                            fit.measures = c("chisq","df","pvalue",
                                                             "rmsea", "cfi", "srmr"))), 
                    digits=3)


# AIC models
out[1,8] <- round(AIC(all_sample),3)
out[2,8] <- round(AIC(configural_cfa),3)
out[3,8] <- round(AIC(metric_cfa),3)

# BIC models
out[1,9] <- round(BIC(all_sample),3)
out[2,9] <- round(BIC(configural_cfa),3)
out[3,9] <- round(BIC(metric_cfa),3)

out[1:3,1] <-  c("Overall CFA", "Configg MG-CFA", "Metric MG-CFA")
```

## Model Parameter Summary

Reporting standardized coefficients.

### All Sample CFA model 

```{r}
##### Summarizing All Samples CFA model #####
parameters(all_sample, standardize = T)
```

### Configural CFA model 

```{r}
##### Summarizing Configural MG-CFA model #####
parameters(configural_cfa, standardize = T)
```

### Metric CFA model 

```{r}
##### Summarizing Metric Multi-group CFA model #####
parameters(metric_cfa, standardize = T)
```

## Comparing models w/ BIC/AIC (anova)

The below compares whether the complete data (across all three samples) in the `all_cfa` model is significantly improved by the configural invariance model. A significant value indicates that the configural model is significantly better than the full sample cfa.

```{r}
anova(all_sample, configural_cfa)
```

Next, anova is used to compare the model improve in AIC/BIC by between the configural and metric invariance. A significantly result in the anova would indicate a significant improvement of the metric model over the configural model.
```{r}
anova(configural_cfa, metric_cfa)
```
## Plotting multi-group config. CFA

Use [semPaths](https://www.rdocumentation.org/packages/semPlot/versions/1.1.6/topics/semPaths) to plot the configural invariance CFA multigroup model

```{r}
# this plottinig is not function with runs loading onto ROIs

layout(t(1:3))
semPaths(configural_cfa,
         color = "lightyellow",
         theme="colorblind",
         whatLabels = "std",
         style = "lisrel",
         sizeLat = 10,
         sizeLat2 = 10,
         sizeMan = 6,
         edge.color = "steelblue",
         edge.label.cex = 2,
         label.cex = 2,
         rotation = 2,
         layout = "tree2",
         intercepts = TRUE,
         residuals = FALSE,
         #residScale = 10,
         curve = 2,
         title = T,
         title.color = "black",
         cardinal = "lat cov",
         curvePivot = T,
         nCharNodes = 6,
         #nodeLabels = label,
         mar = c(2,5,2,6))
# Title 
title("Multi-group CFA on MID task Contrasts")
```


# Running [semi-restricted] ESEM Model {.tabset}

As described in the manuscript, the restricted CFA may incorrectly account for some measurement error in the items. This may degrade the fit statistics. See [Marsh et al. (2014)](www.doi.org/10.1146/annurev-clinpsy-032813-153700) for an in-depth discussion. 

In this case, Exploratory Structural Equation Modeling (ESEM) is used to fit a CFA pre-specified model that allows for non-zero loadings. The technique and application of ESEM is available through the [psych esem](https://www.rdocumentation.org/packages/psych/versions/2.2.5/topics/esem) and [esemcomp](https://msilvestrin.me/post/esemcomp/) package. Here, the `esemcomp` package is used to fit a model using the steps described by Mateus Silvestrin [here](https://msilvestrin.me/post/esem/) and by [Guàrdia-Olmos et al.](https://www.statistics.gov.hk/wsc/CPS105-P6-S.pdf). The github code for esemcomp is available [here](https://mateuspsi.github.io/esemComp/index.html). Below can be used to download the esemComp package -- which worked with R version 4.2.1 on x86_64-apply-dawin17.0 during September 2022.

`devtools::install_github("MateusPsi/esemComp", build_vignettes = TRUE)`

## Selected items for ESEM 

First, select the items that are consistent with those in the CFA model
```{r}
# ordering so can specify numerically
esem_data = brain_set[,c("AWin_v_Neut_L_NAcc_run1"  ,"AWin_v_Neut_L_NAcc_run2" ,
                         "BWin_v_Neut_L_NAcc_run1"  ,"BWin_v_Neut_L_NAcc_run2" ,
                         "BWin_v_BLose_L_NAcc_run1" ,"BWin_v_BLose_L_NAcc_run2",
                          "AWin_v_Neut_R_NAcc_run1" , "AWin_v_Neut_R_NAcc_run2",
                          "BWin_v_Neut_R_NAcc_run1" , "BWin_v_Neut_R_NAcc_run2",
                          "BWin_v_BLose_R_NAcc_run1", "BWin_v_BLose_R_NAcc_run2",
                         # insula values apprach 
                         "AWin_v_Neut_R_Insula_run1","AWin_v_Neut_R_Insula_run2", 
                         "BWin_v_Neut_R_Insula_run1","BWin_v_Neut_R_Insula_run2", 
                         # avoidance
                         "ALose_v_Neut_L_Insula_run1","ALose_v_Neut_L_Insula_run2",
                         "BLose_v_Neut_L_Insula_run1","BLose_v_Neut_L_Insula_run2",
                         "BLose_v_BWin_L_Insula_run1","BLose_v_BWin_L_Insula_run2",
                         "ALose_v_Neut_R_Insula_run1","ALose_v_Neut_R_Insula_run2",
                         "BLose_v_Neut_R_Insula_run1","BLose_v_Neut_R_Insula_run2",
                         "BLose_v_BWin_R_Insula_run1","BLose_v_BWin_R_Insula_run2",
                         "set")]
```



## Specify EFA Model 

As described in March et al. (2014), create a target rotation for items onto factors. In this case two factors are specified by the CFA model, so factor 1 and factor 2 are specified in `make_target`.

```{r}
target_rot <- make_target(28,mainloadings = list(f1 = 1:16, f2 = 17:28))
esem.efa <- esem_efa(data = esem_data[,1:28], nfactors = 2,
                     target = target_rot, fm = "ml")

esem.efa$loadings
```

Using item that loads highest on factor 1 and lowest on factor 2 and vice versa, and define as anchor using `find_referents`
```{r}
# per the example from Mateus Silverstrin, need to define anchor for each factor (value to loads highers on 1 factor and lowest on other)
anchor <- find_referents(efa_object = esem.efa,factor_names = c("f1","f2"))
```


Once the esem efa and anchors are defined, use `syntax_composer` to specied the esem model. This will produce a lavaan specified model that references starting values that will be used in the cfa model
```{r}
# Pull starting parameters
esem_mid_model <- syntax_composer(efa_object = esem.efa, referents = anchor)
```

## Run ESEM model


### Specified Model

The starting values are printed below to provide reference for how starting values differ from a strict CFA model. Notice, how some values that were original not fit onto the Approach factor (f1), such as big lose contrasts, they are now specified with loading values that are between .05 to -.05.

```{r}
cat(esem_mid_model)
```


### Running full ESEM model 
After the EFA loadings are extracted using a target rotation, starting values are now available. These are now used to specify a less restrictive CFA model
```{r}
esem_mid_fit<- cfa(esem_mid_model, esem_data[,1:28], std.lv=TRUE, meanstructure = TRUE,
                   estimator = "MLR")

```

Pull and add fit statistics to the `out` dataframe and print results to see decreases in AIC/BIC
```{r}
# adding values to the CFA model fit indices
out[4,2:7] <- round(data.matrix(fitmeasures(esem_mid_fit, 
                                            fit.measures = c("chisq","df","pvalue",
                                                             "rmsea", "cfi", "srmr"))), 
                    digits=3)
out[4,8] <- round(AIC(esem_mid_fit),3)
out[4,9] <- round(BIC(esem_mid_fit),3)
out[4,1] <-  c("Overall ESEM")

out <- as.data.frame(out)

out %>% 
  knitr::kable(
    col.names = c("Model", "Chi-sq", "DF", "p value", "RMSEA", "CFI", "SRMR", "AIC", "BIC"),
    caption = "Fit statistics from MG-CFA and ESEM models",
    booktabs = TRUE
    )
```


# Aim: 2. Running EFA [Unrestricted] model {.tabset}

Here, a data-driven exploratory factor analysis is performed as implemented using the (https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/factanal)[https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/factanal] in the stats package. The same variables as in the CFA and ESEM dataset are used. A tutorial from Nilam Ram on EFA is also available [here](https://quantdev.ssri.psu.edu/tutorials/intro-basic-exploratory-factor-analysis) 

## Rec. # Factors

A number of methods can be used to estimate the recommended data driven factors in the data. There are nuanced differences in calculations between packages and methods. Therefore, a number of packages are used to acquire consistent evidence to acquire the most parsimonious model.

Using [nFactors](https://cran.r-project.org/web/packages/nFactors/nFactors.pdf) package see the recommended factors for the EFA model using a number of models.

```{r}
fa_data <- subset(esem_data[,1:28])
par(mfrow=c(1,1))
fa.parallel(fa_data) # https://cran.r-project.org/web/packages/nFactors/nFactors.pdf
```

```{r}
plot(nScree(x=esem_data[,1:28],model="factors"))

```

To avoid biasing of packages in different calculations of recommendation factors that depend on strong correlations between bilateral regions by using [parallel analysis](www.doi.org/10.1080/00273170902938969). Parallel analysis is also implemented using the [paran package](https://CRAN.R-project.org/package=paran). 

```{r}
paran(x = esem_data[,1:28],
      iterations = 1000, quietly = FALSE, centile = 95, 
      status = FALSE, all = TRUE, cfa = TRUE, graph = TRUE, color = TRUE, 
      col = c("black", "red", "blue"), lty = c(1, 2, 3), lwd = 1, legend = TRUE, 
      seed = 100)
```


Comparing the above with the BIC comparison of an EFA model to determine the best fitting model based on fit statistics. Factor Analysis is submitted across a range of factors, e.g., 1-5, and the BIC is extracted from the model to determine the optimal number of factors

```{r}
rec_factors <- matrix(NA, ncol = 2, nrow = 20)
colnames(rec_factors) <- c("Nfactors","BIC")

for (f in 1:20) {
  test_fac <- fa(r = esem_data[,1:28],  #raw data  
            nfactors = f, 
            rotate = "promax")
  rec_factors[f,1] <- f
  rec_factors[f,2] <-test_fac$BIC
}

bic_fact = as.data.frame(rec_factors)
```

```{r}
lowest_bic <- which.min(bic_fact$BIC)

bic_fact %>% 
  ggplot(aes(x = Nfactors, y = BIC)) +
  geom_line(colour = 'black', linetype = 'dashed') +
  geom_vline(xintercept = bic_fact$Nfactors[lowest_bic], colour = 'red')+
  theme_minimal()
```


## By Sample EFA

Used the (factanal)[https://www.rdocumentation.org/packages/stats/versions/3.6.2/topics/factanal] to run EFA model. Specifying the number of factors and using the `promax` (non-orthogonal) rotation.

### Sample: *ABCD*
```{r}
abcd_efadata = subset(esem_data %>% filter(set==1))

abcd_efa <- factanal(x = abcd_efadata[,1:28],  #raw data  
              factors = 2, rotation = "promax" # oblique rotation allow for non-orthogonal structure
              )
```

```{r}
heatmaply(round(abcd_efa$loadings[,1:2],2) %>% print(sort = T),
          scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
                 low = "blue", 
                 high = "darkred", 
                 space = "Lab",
                 midpoint = 0, 
                 limits = c(-1, 1)
               ),
               dendrogram = "none",
               xlab = "", ylab = "", 
               main = "",
               margins = c(60,100,40,20),
               grid_color = "white",
               grid_width = 0.00001,
               titleX = FALSE,
               hide_colorbar = FALSE,
               branches_lwd = 0.1,
               label_names = c("Brain:", "Feature:", "Value"),
               fontsize_row = 9, fontsize_col = 9,
               labCol = colnames(abcd_efa$loadings[,1:2]),
               labRow = rownames(abcd_efa$loadings[,1:2]),
               heatmap_layers = theme(axis.line=element_blank()),
          
)
```


### Sample: *MLS*
```{r}
mls_efadata = subset(esem_data %>% filter(set==2))

mls_efa <- factanal(x = mls_efadata[,1:28],  #raw data  
              factors = 2, rotation = "promax" # oblique rotation allow for non-orthogonal structure
              )
```

```{r}
heatmaply(round(mls_efa$loadings[,1:2],2) %>% print(sort = T),
          scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
                 low = "blue", 
                 high = "darkred", 
                 space = "Lab",
                 midpoint = 0, 
                 limits = c(-1, 1)
               ),
               dendrogram = "none",
               xlab = "", ylab = "", 
               main = "",
               margins = c(60,100,40,20),
               grid_color = "white",
               grid_width = 0.00001,
               titleX = FALSE,
               hide_colorbar = FALSE,
               branches_lwd = 0.1,
               label_names = c("Brain:", "Feature:", "Value"),
               fontsize_row = 9, fontsize_col = 9,
               labCol = colnames(mls_efa$loadings[,1:2]),
               labRow = rownames(mls_efa$loadings[,1:2]),
               heatmap_layers = theme(axis.line=element_blank()),
          
)
```

### Sample: *AHRB*
```{r}
ahrb_efadata = subset(esem_data %>% filter(set==3))

ahrb_efa <- factanal(x = ahrb_efadata[,1:28],  #raw data  
              factors = 2, rotation = "promax" # oblique rotation allow for non-orthogonal structure
              )
```

```{r}
heatmaply(round(ahrb_efa$loadings[,1:2],2) %>% print(sort = T),
          scale_fill_gradient_fun = ggplot2::scale_fill_gradient2(
                 low = "blue", 
                 high = "darkred", 
                 space = "Lab",
                 midpoint = 0, 
                 limits = c(-1, 1)
               ),
               dendrogram = "none",
               xlab = "", ylab = "", 
               main = "",
               margins = c(60,100,40,20),
               grid_color = "white",
               grid_width = 0.00001,
               titleX = FALSE,
               hide_colorbar = FALSE,
               branches_lwd = 0.1,
               label_names = c("Brain:", "Feature:", "Value"),
               fontsize_row = 9, fontsize_col = 9,
               labCol = colnames(ahrb_efa$loadings[,1:2]),
               labRow = rownames(ahrb_efa$loadings[,1:2]),
               heatmap_layers = theme(axis.line=element_blank()),
          
)
```


## Quant. Convergence

Calculating a coefficient of factor congruence across the three sample's EFA models. Using function [fa.congruence](https://search.r-project.org/CRAN/refmans/psych/html/factor.congruence.html)

```{r}
fa.congruence(x = list(abcd_efa, mls_efa, ahrb_efa), digits = 2) %>% 
  knitr::kable(
    col.names = c("1. ABCD F1", "2. ABCD F2", "3. MLS F1", "4. MLS F2","5. AHRB F1", "6. AHRB F2"),
    caption = "ABCD, MLS and AHRB EFA Factor Congruence",
    booktabs = TRUE
    )
```

# Running Local SEM {.tabset}

Running CFA for the pubertal variables in the ABCD sample using the local SEM framework described in [Olaru et al (2020)](https://psyarxiv.com/q79c5/) implemented using the [sirt package](https://www.rdocumentation.org/packages/sirt/versions/3.12-66)

## Run LSEM

Specifying the model for the ABCD data below. For now, using the CFA model. In future [real data] implementation, will apply the EFA CFA from n = 1000 ABCD sample in the held out n = 1000 ABCD sample. To pilot, simulating a [fake] pubertal variable that is 1 to 5, as is expected in the Pubertal Developmental Scale.

```{r message=FALSE, warning=FALSE}
#first adding random PDS variable
sim_ABCD_data$PDS <- as.integer(rtnorm(n=1000, mean = 3.5, sd = 1.5, 
                                       lower = 1, upper = 5))

lsem.MID <- sirt::lsem.estimate(data = sim_ABCD_data, moderator = 'PDS', # moderator variable
                                moderator.grid = seq(1,5,1), # moderator levels, PDS 1 - 5
                                lavmodel = MID_model, # model
                                h = 2, # bandwidth parameter 
                                residualize = FALSE, # allow mean level differences 
                                meanstructure = TRUE,
                                std.lv=TRUE
                                )
```

## Summary LSEM

Summarizing output of the `lsem.estimate`

```{r}
summary(lsem.MID)
```

## Plot LSEM

Plotting the `lsem.estimate` for the first 20 indexes.
```{r}

plot(lsem.MID, parindex=1:20)
```

## Permutation Test LSEM

Running permutation test of LSEM model. In this case, using 10 permutation to save on time. In future iterations, permutations will be 1000.


```{r message=FALSE, warning=FALSE}
lsem.permuted <- sirt::lsem.permutationTest(lsem.object = lsem.MID,
                                            B = 10, # permutations 
                                            residualize = FALSE) 
summary(lsem.permuted) # examine results
```





